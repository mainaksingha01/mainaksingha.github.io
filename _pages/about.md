---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hi, I am an AI Research Scientist, working on [Tokyo Research Center](https://www.aisin.com/jp/technology/rd/trc/), [Aisin Corporation](https://www.aisin.com/en), Tokyo, Japan. Aisin is a Fortune Global 500 company and a member of the Toyota Group of companies. My research area in Aisin is mainly focused on Computer-Vision based Machine Perception research in Autonomous Driving.
### Updates

<div style="height:400px;overflow:auto">
<table rules=none style="border:0 none;">
<col width="100px">
<col width="636px">
<tr><td style="border:0 none;"><b>Sep 2023:</b></td><td style="border:0 none;">Best Paper Award again! This time for <a href="https://arxiv.org/abs/2311.15812">C-SAW</a> in ICVGIP'23 </td></tr>

<tr><td style="border:0 none;"><b>Oct 2023:</b></td><td style="border:0 none;"><a href="https://openaccess.thecvf.com/content/WACV2024/html/Bose_STYLIP_Multi-Scale_Style-Conditioned_Prompt_Learning_for_CLIP-Based_Domain_Generalization_WACV_2024_paper.html"> StyLIP</a> is accepted in WACV 2024 </td></tr>

<tr><td style="border:0 none;"><b>Oct 2023:</b></td><td style="border:0 none;"><a href="https://arxiv.org/abs/2311.15812">C-SAW</a> a self-supervised VLM in Remote Sensing, is accepted in ICVGIP 2023 </td></tr>
  
<tr><td style="border:0 none;"><b>Oct 2023:</b></td><td style="border:0 none;">Joined <a href="https://www.aisin.com/jp/technology/rd/trc/">TRC</a>, Aisin Corporation in Tokyo, Japan!</td></tr>

<tr><td style="border:0 none;"><b>Sep 2023:</b></td><td style="border:0 none;">HAVE-Net has received Best Paper Award in ECML-PKDDw'23, Turin, Italy !! </td></tr>

<tr><td style="border:0 none;"><b>Aug 2023:</b></td><td style="border:0 none;">Work on domain adaptation using Prompt Learning based model <a href="https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Singha_AD-CLIP_Adapting_Domains_in_Prompt_Space_Using_CLIP_ICCVW_2023_paper.html"> AD-CLIP </a> is accepted in ICCVw 2023 </td></tr>

<tr><td style="border:0 none;"><b>Aug 2023:</b></td><td style="border:0 none;"><a href="https://papers.bmvc2023.org/0314.pdf"> GOPro</a>, for self-supervised domain generalization, is accepted in BMVC 2023 </td></tr>

<tr><td style="border:0 none;"><b>July 2023:</b></td><td style="border:0 none;">Hallucinated Audio-Visual Embeddings <a href="[https://papers.bmvc2023.org/0314.pdf](https://arxiv.org/abs/2309.13470)"> HAVE-Net </a> is accepted in ECML-PKDDw 2023 </td></tr>

<tr><td style="border:0 none;"><b>April 2023:</b></td><td style="border:0 none;">Paper on <a href="https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Jha_APPLeNet_Visual_Attention_Parameterized_Prompt_Learning_for_Few-Shot_Remote_Sensing_CVPRW_2023_paper.html">APPLeNet</a> : a Vision-Language model (VLM) in Remote Sensing, is accepted in CVPRw 2023 </td></tr>
